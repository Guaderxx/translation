# Redis的分布式锁

> Redis的分布式锁模式
> [原文](https://redis.io/docs/manual/patterns/distributed-locks/)

在不同进程必须以互斥方式使用共享资源的很多环境中，分布式锁是一个非常有用的原语。

有很多的库和博客文章描述了怎样用Redis实现一个DLM（Distributed Lock Manager)。
但是每个库用了不同的方法，很多库使用简单的方法，与稍微复杂的设计可以实现的相比，保证性较低。

这个页面描述了一个更规范的用Redis实现分布式锁的算法。
我们提出了称为Redlock的算法，它实现了我们认为比普通单实例方法更安全的DLM。
我们希望社区对其进行分析，提供反馈，并将其用作实现或更复杂或替代设计的起点。



## 实现

[实现](https://redis.io/docs/manual/patterns/distributed-locks/#implementations)

- [Redsync](https://github.com/go-redsync/redsync)  Go implementation.
- [node-redlock](https://github.com/mike-marcacci/node-redlock)  NodeJS implementation. Incluted support for lock extension.



## 安全性和有效性的保证

我们将仅使用三个属性对我们的设计进行建模，从我们的角度看，
这三个属性是有效使用分布式锁所需的最低保证。

1. 安全性：互斥。在任何给定时刻，只有一个客户端可以持有一把锁。
2. 有效性A：无死锁。最终总是可以获得锁，即使锁定资源的客户端崩溃或是被分割。
3. 有效性B：容错。只要大多数Redis节点正常运行，客户端就可以获取和释放锁。



## 为什么基于故障转移的实施是不够的

为了理解我们想要提高什么，让我们分析一下大多数基于Redis的分布式锁库的现状。

最简单的使用Redis锁定资源的方式是在一个实例中创建一个键。
这个键通常创建时附带了有效期，通过Redis expire功能，所以最后肯定可以被释放。
当客户端需要释放资源时，它会删除这个键。

表面上这很好，但是有一个问题：这个我们架构中的单点故障。
如果Redis主节点挂掉了怎么办？让我们添加一个从节点。在主节点不可用时使用它。
不幸的是，这是不可行的。这样我们无法实现互斥的安全性，因为Redis复制是异步的。

这个模型存在竞争条件：

1. 客户端A从主节点获取了锁
2. 主节点在将键传输到从节点前崩了
3. 从节点被提升为主节点
4. 客户端B获取了A已经持有锁的这个资源的锁。**违反安全规定**

有时在特殊情况下，例如在故障期间，多个客户端可以同时持有锁是完全没问题的。
如果是这种情况，你可以用基于复制的解决方案。否则，建议实施本文档中描述的解决方案。



## 单实例的正确实现

在尝试克服上述单实例设置的限制之前，让我们检查一下如何在这个简单的情况下正确的做这件事，
因为这实际上是一个可行的解决方案，在应用中不时出现的竞赛条件是可以接受的，而且因为
锁定到一个单实例是我们将用于这里描述的分布式算法的基础。

像下面这样获取锁：

```redis-cli
SET resource_name my_random_value NX PX 30000
```

这个命令只有在键不存在时（NX 选项）起效，过期事件为30000ms（PX 选项）。
这个键设置了值`my_random_value`。
这个值在所有的客户端和所有的锁请求里必须是唯一的。

基本上，使用随机值是为了以安全的方式释放锁，用脚本告诉Redis:
只有在键存在且值为给定值的时候才移除。
这是通过下面的Lua脚本完成的：

```lua
if redis.call("get", KEYS[1]) == ARGV[1] then
    return redis.call("del", KEYS[1])
else
    return 0
end
```

这很重要，避免删除由另一个客户端创建的锁。
例如，客户端可能会获取锁，在执行某些操作时被阻塞的时间超过锁的有效期（密钥将过期的时间），
然后删除已被其他客户端获取的锁。
仅使用`DEL`是不安全的，因为客户端可能会删除另一个客户端的锁。
使用上面的脚本，每个锁都用一个随机字符串“签名”，所有只有当它仍然是客户端试图删除它设置的锁时才会被删除。

这个随机字符串应该是什么？我们假设它是`/dev/urandom`的20个字节，但你可以找到更便宜的方法来使其对你的任务足够独特。
例如，一个安全的选择是使用`/dev/urandom`作为RC4的种子，并从中生成一个伪随机流。
一个更简单的解决方案是使用具有微秒精度的UNIX时间戳，将时间戳和客户端ID连接起来。
它不那么安全，但对于大多数环境来说可能已经足够了。

“lock validity time”就是我们用的键的存活时间。
同时也是自动释放的时间，以及客户端在另一个客户端可能再次获取锁之前执行所需操作的时间。
在技术上不违反互斥保证，这只限于从获得锁的那一刻起的特定时间窗口。

那么现在我们有一个很好的方法来获取和释放锁。
有了这个系统，关于由一个单一的，始终可用的实例组成的非分布式系统的推理是安全的。
让我们将概念扩展到我们没有此类保证的分布式系统。



## Redlock算法

在这个算法的分布式版本，我们假设有N个Redis主节点。
这些节点是完全独立的，所以我们不使用从节点或任何其他隐含的协调系统。
我们已经描述了怎样从单实例安全的获取和释放锁。
我们理所当然的认为算法会使用这个方法在单实例中获取和释放锁。
在我们的例子中，我们将N设置为5,这个一个合理的值，所以我们需要在不同的主机或是虚拟机上运行这5个Redis节点。
确保它们会以一种基本独立的方式失败。

为了获取锁，客户端执行以下操作：

1. 获取当前的毫秒时间
2. 它尝试依次获取所有N个实例中的锁，在所有实例中使用相同的键名和随机值。
   在步骤2中，在每个实例中设置锁时，客户端使用与总锁自动释放时间相比较小的超时来获取锁。
   例如如果自动释放时间为10S，超时时间会有5-50MS的波动。
   这可以防止客户端在尝试与已关闭的Redis节点通信时长时间处于阻塞状态：如果一个实例不可用，我们应该尽快尝试与下一个实例通信。
3. 客户端通过从当前时间减去步骤1中获得的时间戳来计算获得锁所用的时间。
   当且仅当客户端能够在大多数实例（至少3个）中获取到锁，并且获取锁花费的总时间小于锁有效时间，才认为成功获取了锁
4. 如果获取了锁，则其有效时间被认为是初始有效时间减去经过的时间，如步骤3中计算的那样。
5. 如果客户端由与某种原因没有获得锁（可能是无法锁定N/2+1个实例，或是有效期为负）
   它会尝试解锁所有实例（即使是它认为无法锁定的实例）。



## 这个算法是异步的么？

该算法依赖于这样的假设，即虽然各进程之间没有同步时钟，但每个进程的本地时间都以大致相同的速度更新，
与锁的自动释放时间相比，误差很小。
这个假设非常类似于真实世界的计算机：每台计算机都有一个本地时钟，我们通常可以依靠不同的计算机来实现很小的时钟漂移。

在这一点上，我们需要更好的说明我们的互斥规则：只要持有锁的客户端在锁的有效期内（如在步骤3中得到的）终止其工作，
减去一些时间（只是几毫秒，以补偿进程之间的时钟偏移），就可以保证。

这个论文包含了更多关于需要约束时钟漂移的类似系统的信息：
[租约：分布式文件缓存一致性的高效容错机制](http://dl.acm.org/citation.cfm?id=74870)



## 失败重试

当一个客户端无法获取锁时，它应该在一个随机的延迟后再次尝试，
以尝试对试图在同一时间为同一资源获取锁的多个客户端进行异步处理
（这可能会导致大脑分裂的情况，没有人获胜）。
另外，客户端试图在大多数Redis实例中获取锁的速度越快，大脑分裂情况的窗口就越小（以及重试的需要），
所以理想情况下，客户端应该尝试使用复用技术同时向N个实例发送SET命令。

值得强调的是，对于未能获得大多数锁的客户来说，尽快释放（部分）获得的锁是多么重要，
这样就不需要等待密钥过期，以便再次获得锁（然而，如果发生网络分区，客户不再能够与Redis实例通信，
在等待密钥过期的过程中，需要支付可用性罚款）。



## 释放锁

释放锁很简单，无论客户端是否认为它能够成功锁定给定实例都可以执行。



## 安全论证

这个算法安全么？让我们探索不同场景下会发生什么。

开始让我们假设一个客户端可以获取大部分实例上的锁。
所有的实例上都包含了一个存活时间相同的键。
但是，这个键是不同时间设置的，所以它会在不同的时间过期。
但是，如果第一个键在最坏的情况下被设置为T1（我们联系第一台服务器之前的采样时间），
而最后一个键在最坏的情况下被设置在T2（从最后一个服务器获得响应的时间），
我们可以肯定，这个集合中第一个过期的键将至少存在`MIN_VALIDITY=TTL-(T2-T1)-CLOCK_DRIFT`。
所有其他的键都会在之后过期，因此我们确信至少这次将同时设置这些键。

在大部分的键已经被设置时，另一个客户端将无法获取锁，也就是如果`N/2 + 1`的键已经设置了，那么`N/2 + 1 SET NX`操作不能成功。
所以如果已经获取了锁，同一时间时不可能再次获取该锁的（违反互斥性质）。

但是，我们还想确保多个客户端在同一时刻一起获取锁的操作不能同时成功。

如果一个客户端使用接近或大于锁的最大有效时间（基本上我们用于SET的TTL）锁定了大多数实例，
它将认为该锁无效并将解锁这些实例，所以我们只需要考虑客户能够在小于有效时间的时间内锁定大多数实例的情况。
这种情况下，对于上面已经表达的参数，根据`MIN_VALIDITY`，没有客户端应该能够重新获取锁。
因此，多个客户端能够同时锁定`N/2 + 1`个实例（“时间”是步骤2的结束），
只有当大多数的锁的时间大于TTL是，才会使锁无效。



## 有效性论证

这个系统的有效型基于三个主要功能：

1. 锁的自动释放（如果键过期了）：最终键都可以被再次上锁
2. 事实上，客户端通常会在没有获得锁的时候，或者在获得了锁而工作终止的时候，
   配合移除锁，使得我们很可能不必等待钥匙过期来重新获取锁。
3. 事实上，当客户端需要重新获取一个锁时，它所等待的时间要比获得大多数锁所需的时间大得多，
   以便在概率上使资源争夺期间的大脑分裂情况不太可能发生。

然而，我们在网络分区上支付的可用性惩罚等于TTL时间，
所以如果有连续的分区，我们可以无限期的支付这个惩罚。
每次客户端获取锁时，或是在能够移除锁之前被分区时，就会发生这种情况。

基本上，如果有无限的连续网络分区，系统可能在无限长的时间内变得不可用。



## 性能，崩溃恢复和fsync

许多使用Redis作为锁服务的用户需要高性能，包括获取和释放锁的延迟，以及每秒钟可能执行的获取/释放操作的数量。
为了满足这一要求，与N个Redis服务器对话以减少延迟的策略肯定是多路复用
（将套接字置于非阻塞模式，发送所有的命令，随后读取所有的命令，假设客户端和每个实例之间的RTT是相似的）。

但是，如果我们想以崩溃恢复系统模型为目标，围绕持久性还有另一个考虑。

基本上要看到这里的问题，让我们假设我们在配置Redis时根本没有持久性。
一个客户端在5个实例中的3个获取了锁。
客户端获取锁的其中一个实例被重新启动，此时又有三个实例可以让我们为同一资源加锁，
另一个客户端可以再次加锁，违反了锁的排他性的安全属性。

如果我们启用了AOF持久化，这里会有点改进。
例如我们可以通过发送**SHUTDOWN**命令来重启并升级服务器。
因为Redis过期是语义实现的，所以当服务器关闭时时间仍然在流逝，我们的所有需求都很好。
但是，只要是干净关闭，一切都很好。
断电了会怎样？
如果Redis设置了配置，默认情况下，每秒向硬盘同步，重启后我们的键可能是会丢失的。
理论上，如果我们想要保证在任何实例重启的情况下键的安全，我们需要在持久化设置中启用`fsync=always`。
由于额外的同步开销，这会影响性能。

然而事情比乍看之下好得多。
基本上，只要实例在崩溃后重新启动时，算法安全性就会得到保留，它不再参与任何当前**活动**的锁。
这意味着当实例重新启动时，当前活动锁的集合都是通过锁的实例而不是重新加入系统的实例获得的。

为了保证这一点，我们只需要让一个实例在崩溃后不可用，至少比我们使用的最大TTL长一点。
这是实例崩溃时存在的关于锁的所有键变得无效并被自动释放所需的时间。

使用*延迟重启*，即使没有任何可用的Redis持久性，基本上也可以实现安全，
但请注意，这可能会转化为可用性损失。
例如如果大多数实例崩溃，系统将在TTL下变得全局不可用（这里的全局意味着在此期间根本没有资源可锁定）。



## 使算法更可靠：扩展锁

如果客户端执行的工作由小步骤组成，可以默认使用更小的锁有效期，并扩展实现锁扩展机制的算法。
基本上，如果在计算过程中锁的有效性接近一个低值时，客户端可以通过向所有的实例发送一个Lua脚本来扩展锁，
如果键存在且它的值仍然是客户端在获取锁时分配的随机值，则扩展锁。

客户端只有在能够将锁扩展到大多数实例，并且在有效期内
（基本上使用的算法与获取锁时使用的算法非常近似），才应该考虑重新获取锁。

然而，这在技术上没有改变算法，因此应该限制重新获取锁尝试的最大次数，否则会违反其中一个有效性属性。



## 关于一致性的免责声明

请考虑彻底查看本页末尾的[Redlock分析](https://redis.io/docs/manual/patterns/distributed-locks/#analysis-of-redlock)部分。
Martin Kleppman 的文章和 antirez 的回答非常相关。
如果你担心一致性和正确性，则应注意以下主题：

1. 你应该实现防护令牌。这对于可能花费大量时间并适用于任何分布式锁系统的进程尤其重要。
   延长锁的生命周期也是一种选择，但不要假设只要获得锁的进程还活着，锁就会一直保留。
2. Redis没有为TTL过期机制使用单调时钟。这意味着时钟偏移可能导致一个锁被多个进程获取。
   尽管可以通过阻止管理员手动设置服务器时间与正确的设置NTP来缓解这个问题，
   但这个问题仍然可能在现实生活中发生并损害一致性。



## Redlock分析

Martin Kleppman在[这里分析了Redlock](http://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html)。可以在[此处](http://antirez.com/news/101)找到与此分析相对应的观点。
